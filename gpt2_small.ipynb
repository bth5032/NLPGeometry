{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, numpy as np, pandas as pd\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (GPT2Model,GPT2LMHeadModel, \n",
    "                          GPT2Config, GPT2Tokenizer,\n",
    "                         BertConfig, BertTokenizer,\n",
    "                         BertModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary encoding pretrained (config, tokenizer, model) options we might consider:\n",
    "language_model_dict = {'bert-base-uncased': [BertConfig, BertTokenizer, BertModel],\n",
    "             'bert-base-multilingual-cased': [BertConfig, BertTokenizer, BertModel],\n",
    "              'gpt2': [GPT2Config, GPT2Tokenizer, GPT2LMHeadModel],\n",
    "              'gpt2-xl': [GPT2Config, GPT2Tokenizer, GPT2Model]\n",
    "             }\n",
    "\n",
    "# Dictionary encoding pretrained (config, tokenizer, model) options we might consider:\n",
    "model_dict = {'bert-base-uncased': [BertConfig, BertTokenizer, BertModel],\n",
    "             'bert-base-multilingual-cased': [BertConfig, BertTokenizer, BertModel],\n",
    "              'gpt2': [GPT2Config, GPT2Tokenizer, GPT2LMHeadModel],\n",
    "              'gpt2-xl': [GPT2Config, GPT2Tokenizer, GPT2LMHeadModel]\n",
    "             }\n",
    "\n",
    "# Choose a huggingface pretrained model from the list above, and maybe other options moving forward\n",
    "config = {\"model_name\":'gpt2'}\n",
    "\n",
    "# Choose a pretrained mode\n",
    "pretrained_model = config[\"model_name\"]\n",
    "\n",
    "# Set device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a pretrained gpt2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = model_dict[pretrained_model][1].from_pretrained(pretrained_model)\n",
    "\n",
    "# Load configuration for bert model, and set all hidden states to be output\n",
    "config = model_dict[pretrained_model][0].from_pretrained(pretrained_model, output_hidden_states=True, use_cache = False, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "# Load pretrained bert with desired config\n",
    "model = model_dict[pretrained_model][2].from_pretrained(pretrained_model, config = config)\n",
    "model.eval()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Language Modeling usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Use model.generate wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize some input\n",
    "tokens = tokenizer.encode(\"Can you guess what I am going to say\", return_tensors='pt')\n",
    "tokens = tokens.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Can you guess what I am going to say? That's pretty bad.\\n\\nWhat's the best word for this kind of thing?\\n\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the model to generate text beginning with previous text as context, \n",
    "# by using top-k decoding within the model.generate wrapper\n",
    "tokenizer.decode(model.generate(tokens, do_sample=True, \n",
    "    max_length=30, top_k = 30)[0],skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Don't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: use the model to generate text beginning with previous text as context,\n",
    "# by using greedy decoding directly from the LMHead model output:\n",
    "\n",
    "# Extract final output layer from the LM\n",
    "with torch.no_grad():\n",
    "    # All outputs from the Language model\n",
    "    outputs = model(tokens)\n",
    "    # The logits output for each of the input tokens:\n",
    "    predictions = outputs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ``predictions`` output gives a score for each element of the vocab, for each of the input tokens. This is documented in the return statement of transformers.GPT2LMHeadModel, as described here: https://huggingface.co/transformers/model_doc/gpt2.html#gpt2lmheadmodel . Our ``predictions`` is the returned ``logits`` in the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9, 50257])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the shape to be sure:\n",
    "# Shape = (batch_size, num_tokens, vocab_size)\n",
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Can you guess what I am going to say?'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Greedily generate the next word by finding the highest scoring vocab\n",
    "# item for the last token in the input.\n",
    "predicted_index = torch.argmax(predictions[0, -1, :]).reshape(1,1)\n",
    "predicted_text = tokenizer.decode(torch.cat((tokens,predicted_index), dim =1).reshape(-1))\n",
    "predicted_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Token Embedding Representations from LM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now chop off the head of this model, and extract the last hidden state as a sort of high level, embedding. Note that by virture of the pretrained model itself, we'll get an embedding per input token in doing this, and we will need to come up with out our own determination of how to associate a single embedding to a sentence. We'll separately instantiate the cut off head as it's own decoder model to read these embeddings. Finally, we'll investigate the effect of negating sentences in two ways:\n",
    " \n",
    "1.  Studying the embeddings themselves, by e.g clustering, studying norm distributions, angle distributions, etc.\n",
    "2.  Considering the differences of the word embeddings associated to (clause, negation) pairs, and then looking at the distribution of sentiments/clauses/phrases occurring when those differences are decoded by the LM head."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Positive and Negative Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A statement and one possible negation.\n",
    "statement = 'This book is bad.'\n",
    "negation = 'This book is good.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hidden_output(text, model = model, tokenizer = tokenizer, layer_num = -1,\n",
    "                         aggregation = 'average'):\n",
    "    \n",
    "    \n",
    "    # Average the embeddings for each token, and return\n",
    "    if aggregation == 'average':\n",
    "        # tokenize the input\n",
    "        tokens = tokenizer.encode(text, return_tensors = 'pt')\n",
    "        tokens = tokens.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # All outputs from the Language model\n",
    "            outputs = model(tokens)\n",
    "            # All hidden states\n",
    "            hidden_states = outputs[1]\n",
    "            # Hidden state from layer layer_num\n",
    "            layer = hidden_states[layer_num]\n",
    "            layer = torch.squeeze(layer)\n",
    "            if len(layer.shape) > 1:\n",
    "                averaged_layer = torch.mean(layer, dim = 0)\n",
    "                return averaged_layer\n",
    "            else:\n",
    "                return layer\n",
    "        \n",
    "        \n",
    "        \n",
    "    # Return the embeddings for each token\n",
    "    if aggregation == None:\n",
    "        tokens = tokenizer.encode(\n",
    "            text, \n",
    "            return_tensors = 'pt'\n",
    "            #,max_length=max_length,\n",
    "            #pad_to_max_length=True\n",
    "        )\n",
    "        tokens = tokens.to(device)\n",
    "        '''\n",
    "        Return all token embeddings, so can study each individually. For any batch processing,\n",
    "        this may require different sentiments have the same number of tokens, so we may want to set\n",
    "        a max length for sentences and pad.\n",
    "        '''\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the cosine of the angle between the last layer embeddings associated to the statement and it's negation above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9998, device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statement_vec = extract_hidden_output(statement).reshape(-1)\n",
    "negation_vec = extract_hidden_output(negation)\n",
    "cosine = torch.dot(statement_vec, negation_vec)/(torch.norm(negation_vec)*torch.norm(statement_vec))\n",
    "cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This small angle for a negation confused me at first. I suppose this is not so surprising, though, and reflects that angle has little to do with similarity in these sorts of transformer models, which in and of itself is interesting, given that it is incredibly pervasive for people to use cosine similarity on hidden states as if that represents similarity. Maybe a better interpretation here is that good and bad have similar angles because they are very likely to occur in similar contexts, and this model was probably trained on MLM. For example, ('turtle', 'bad') have a larger angle between them than either ('good', 'bad') or ('good', 'lovely'), which makes sense in the latter interpretation above.\n",
    "\n",
    "\n",
    "Or, perhaps the way to construct the sentence vector is not the one implemented here, and the correct one actually results in a larger angle and this example is completely inaccurate!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Stanford real-life contradictions dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "tree = ET.parse('data/real_contradiction.xml')\n",
    "root = tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = []\n",
    "for child in root:\n",
    "    pair =[]\n",
    "    attrib = child.attrib\n",
    "    if (attrib['contradiction'] =='YES') & \\\n",
    "    ((attrib['type'] == 'lexical') | (attrib['type'] == 'negation')):\n",
    "        for statement in child:\n",
    "            pair.append(statement.text)\n",
    "    pairs.append(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove empty rows\n",
    "pairs = [pair for pair in pairs if len(pair)>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pairs = pd.DataFrame(pairs, columns = [\"statement\",\"negation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do:\n",
    "1.  Create some negation data or find some, and then study the distributions of some aspects of the embeddings above for (sentence, negation) pairs.\n",
    "2. Extract the LM head and instantiate as a separate model, to use as a decoder for approach 2 above.\n",
    "3.  Determine how to construct sentence embeddings from token embeddings.\n",
    "4.  Reproduce this notebook for Bert, which hopefully should be straightforward aside from the generate wrapper which appears from errors to either not exist or have some different api defaults."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
