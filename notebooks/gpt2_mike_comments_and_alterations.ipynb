{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, numpy as np\n",
    "from transformers import GPT2Model,GPT2LMHeadModel, GPT2Config, GPT2Tokenizer\n",
    "\n",
    "torch.set_grad_enabled(False) #no training in this NB\n",
    "norm=np.linalg.norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Load in the gpt2 XL model\n",
    "Using `GPT2LMHeadModel` allows us to actually decode our vectors back into language. Below we'll just get the vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configuration = GPT2Config()\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\") #use 'gpt2' if you don't want to download 6GB of weights and use 20 GB of ram\n",
    "language_model = GPT2LMHeadModel.from_pretrained('gpt2',pad_token_id=tokenizer.eos_token_id) #use 'gpt2' if you don't want to download 6GB of weights and use 20 GB of ram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'An interesting unexplored thing about language models is that they actually have a rich group structure, for instance, some of which we can think of as a set of words. In such a statement, you might say: \"It is the word of God, but that is not enough. So it should be a list of these words, the whole of which must be different from any others.\" But even for this simple statement, it must be really long. If those words mean a common word, then'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.encode(\"An interesting unexplored thing about language models is that they actually have a rich group structure, for instance\", return_tensors='pt')\n",
    "\n",
    "# top-k decoding\n",
    "tokenizer.decode(language_model.generate(tokens, do_sample=True, \n",
    "    max_length=100, \n",
    "    top_k=50)[0],skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124439808"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#~ 12 million params\n",
    "sum(p.numel() for p in language_model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can probably get the representation at any particular layer using a `pytorch` callback, but for now lets just instantiate another gpt2 model without the \"LMHead\" and see what kind of vectors that gives us. *actually seems like we can get intermediate reps right from the `forward` method*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2Model were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = GPT2Model.from_pretrained('gpt2') #use 'gpt2' if you don't want to download 6GB of weights and use 20 GB of ram\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\") #use 'gpt2' if you don't want to download 6GB of weights and use 20 GB of ram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Error message referred to above seems to be related to this known issue:\n",
    "    https://github.com/huggingface/transformers/pull/5922\n",
    "Perhaps the issue is resolved by the PR referenced in that thread. Try pulling the master branch to see if issue is resolved?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.encode(\"I am great\", return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  40,  716, 1049]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "representation = model(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 768])\n",
      "12\n",
      "torch.Size([2, 1, 12, 3, 64])\n",
      "torch.Size([2, 1, 12, 3, 64])\n"
     ]
    }
   ],
   "source": [
    "print(representation[0].shape)\n",
    "print(len(representation[1]))\n",
    "print(representation[1][0].shape)\n",
    "print(representation[1][1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Not really sure what's being returned. From https://huggingface.co/transformers/pretrained_models.html, it seems the hidden state size for the sentence (i.e. it's embedding) is 1600, and there are 48 layers for this model. So I guess the tuple is \n",
    "- element 0: hidden state\n",
    "- element 1: hidden states at each prior attention block.\n",
    "Looking at the vectors in element 1, it seems the 3rd dimension runs over the \"heads\" as there are 25 for this model. Below just check to make sure the same is true for the smaller gpt2 model. It seems to match, 12 heads, final hidden state is 768.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">I think the above is actually not quite right. See the return statement for transformers.GPT2Model here: https://huggingface.co/transformers/model_doc/gpt2.html#gpt2model. It's hard to tell explicitly what is being returned without looking at the config (we should probably explicitly instantiate configs to avoid this issue), but just from the shape it seems to me we must be viewing an output tuple consisting of (``last_hidden_state``, ``past_key_values``) (see the referred link for definitions of those two things). This explains for example the 2 appearing in the first dimension of ``element 1``; if element 1 was meand to contain hidden states, we should expect the dimension of the hidden layers (in this case 768) to be appearing in some way, but it is not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare some cosines between sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2Model were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model2 = GPT2Model.from_pretrained('gpt2') \n",
    "tokenizer2 = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 768])\n",
      "12\n",
      "torch.Size([2, 1, 12, 5, 64])\n",
      "torch.Size([2, 1, 12, 5, 64])\n"
     ]
    }
   ],
   "source": [
    "tokens2 = tokenizer2.encode(\"I am a great man\", return_tensors='pt')\n",
    "model2.train(False)\n",
    "representation2 = model2(tokens2)\n",
    "print(representation2[0].shape)\n",
    "print(len(representation2[1]))\n",
    "print(representation2[1][0].shape)\n",
    "print(representation2[1][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99922514\n"
     ]
    }
   ],
   "source": [
    "tok_pos = tokenizer2.encode(\"good\", return_tensors='pt')\n",
    "rep_pos = model2(tok_pos)[0].numpy().flatten()\n",
    "tok_neg = tokenizer2.encode(\"bad\", return_tensors='pt')\n",
    "rep_neg = model2(tok_neg)[0].numpy().flatten()\n",
    "\n",
    "print(rep_neg.dot(rep_pos)/(norm(rep_neg)*norm(rep_pos)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12, 768])\n",
      "12\n",
      "torch.Size([2, 1, 12, 12, 64])\n",
      "torch.Size([2, 1, 12, 12, 64])\n",
      "torch.Size([1, 5, 768])\n",
      "12\n",
      "torch.Size([2, 1, 12, 5, 64])\n",
      "torch.Size([2, 1, 12, 5, 64])\n"
     ]
    }
   ],
   "source": [
    "tok_pos = tokenizer2.encode(\"I ate dinner at 5 pm, that's a bit strange\", return_tensors='pt')\n",
    "rep_pos = model2(tok_pos)[0].numpy().flatten()\n",
    "tok_neg = tokenizer2.encode(\"I am the walrus\", return_tensors='pt')\n",
    "rep_neg = model2(tok_neg)[0].numpy().flatten()\n",
    "\n",
    "rep_pos = model2(tok_pos)\n",
    "print(rep_pos[0].shape)\n",
    "print(len(rep_pos[1]))\n",
    "print(rep_pos[1][0].shape)\n",
    "print(rep_pos[1][1].shape)\n",
    "\n",
    "rep_pos = model2(tok_neg)\n",
    "print(rep_pos[0].shape)\n",
    "print(len(rep_pos[1]))\n",
    "print(rep_pos[1][0].shape)\n",
    "print(rep_pos[1][1].shape)\n",
    "#print(rep_neg.dot(rep_pos)/(norm(rep_neg)*norm(rep_pos)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, this is a bit more work than I thought, the `hidden state` is the attention representation for each token (I guess in the previous sentence `am a` must be ignored, which is why the second dimension is length 3). If we want \"sentence representations\" then we will need to figure out how these are combined in the \"decoder\" I guess."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Is there some documentation you know of that makes you think the second dimension is 3 because 'am' and 'a' are dropped? While that doesn't sound unreasonable, I don't know where to find which such words might get dropped, and it seems odd to me given that the documentation doesn't suggest any such list of words exists (that I can see so far). I think instead perhaps you accidentally wrote ``representation2 = model2(tokens)`` when you meant to write ``representation2 = model2(tokens2)``. I altered it above, and didn't notice any dimension counts that indicated dropping of words as you did (in my case, there is a 5 where you had 3, which matches the number of words (and perhaps tokens) in the sentence)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ToDo\n",
    "1. Error messages keep saying we should train a model before running any inference, that's pretty weird, wonder if that's actually neccessary. Need to look into that.\n",
    ">Error message referred to above seems to be related to this known issue:\n",
    "    https://github.com/huggingface/transformers/pull/5922\n",
    "Perhaps the issue is resolved by the PR referenced in that thread. Maybe we should try pulling the master branch and installing the most recent build in that way to see if the issue is resolved? I haven't tried that yet, but plan to. My basic understanding is that the issue is because this api call is pulling weights for a model with a slightly different head than the LM model, so the warning is letting you know the weights aren't necessarily all pretrained as intended (though in this case, I think they are).\n",
    "\n",
    "2. If I want to decode the hidden state of this sentence, I wonder if I can just instantiate one of those `LMHead`s and then try to decode the difference vectors\n",
    "3. What's the natural way to compose the attention reps for each word?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
