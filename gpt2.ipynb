{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, numpy as np\n",
    "from transformers import GPT2Model,GPT2LMHeadModel, GPT2Config, GPT2Tokenizer\n",
    "from pytorch_model_summary import summary\n",
    "\n",
    "torch.set_grad_enabled(False) #no training in this NB\n",
    "norm=np.linalg.norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the gpt2 XL model\n",
    "Using `GPT2LMHeadModel` allows us to actually decode our vectors back into language. Below we'll just get the vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configuration = GPT2Config()\n",
    "language_model = GPT2LMHeadModel.from_pretrained('gpt2-xl') #use 'gpt2' if you don't want to download 6GB of weights and use 20 GB of ram\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-xl\") #use 'gpt2' if you don't want to download 6GB of weights and use 20 GB of ram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'An interesting unexplored thing about language models is that they actually have a rich group structure, for instance each component (e.g. \"this is a word\", \"this is not a word\") is a part of a whole. In our case with Word2Vec, we use embeddings to transform this word-based group structure into the word-free vocabulary structure that we seek (because you can\\'t represent the word-free space as an array of vectors, you have to \"'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.encode(\"An interesting unexplored thing about language models is that they actually have a rich group structure, for instance\", return_tensors='pt')\n",
    "\n",
    "tokenizer.decode(language_model.generate(tokens, do_sample=True, \n",
    "    max_length=100, \n",
    "    top_k=50)[0],skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1557611200"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1.5 billion params\n",
    "sum(p.numel() for p in language_model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can probably get the representation at any particular layer using a `pytorch` callback, but for now lets just instantiate another gpt2 model without the \"LMHead\" and see what kind of vectors that gives us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2Model were not initialized from the model checkpoint at gpt2-xl and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias', 'h.12.attn.masked_bias', 'h.13.attn.masked_bias', 'h.14.attn.masked_bias', 'h.15.attn.masked_bias', 'h.16.attn.masked_bias', 'h.17.attn.masked_bias', 'h.18.attn.masked_bias', 'h.19.attn.masked_bias', 'h.20.attn.masked_bias', 'h.21.attn.masked_bias', 'h.22.attn.masked_bias', 'h.23.attn.masked_bias', 'h.24.attn.masked_bias', 'h.25.attn.masked_bias', 'h.26.attn.masked_bias', 'h.27.attn.masked_bias', 'h.28.attn.masked_bias', 'h.29.attn.masked_bias', 'h.30.attn.masked_bias', 'h.31.attn.masked_bias', 'h.32.attn.masked_bias', 'h.33.attn.masked_bias', 'h.34.attn.masked_bias', 'h.35.attn.masked_bias', 'h.36.attn.masked_bias', 'h.37.attn.masked_bias', 'h.38.attn.masked_bias', 'h.39.attn.masked_bias', 'h.40.attn.masked_bias', 'h.41.attn.masked_bias', 'h.42.attn.masked_bias', 'h.43.attn.masked_bias', 'h.44.attn.masked_bias', 'h.45.attn.masked_bias', 'h.46.attn.masked_bias', 'h.47.attn.masked_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = GPT2Model.from_pretrained('gpt2-xl') #use 'gpt2' if you don't want to download 6GB of weights and use 20 GB of ram\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-xl\") #use 'gpt2' if you don't want to download 6GB of weights and use 20 GB of ram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.encode(\"I am great\", return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  40,  716, 1049]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(False)\n",
    "representation = model(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 1600])\n",
      "48\n",
      "torch.Size([2, 1, 25, 3, 64])\n",
      "torch.Size([2, 1, 25, 3, 64])\n"
     ]
    }
   ],
   "source": [
    "print(representation[0].shape)\n",
    "print(len(representation[1]))\n",
    "print(representation[1][0].shape)\n",
    "print(representation[1][1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not really sure what's being returned. From https://huggingface.co/transformers/pretrained_models.html, it seems the hidden state size for the sentence (i.e. it's embedding) is 1600, and there are 48 layers for this model. So I guess the tuple is \n",
    "\n",
    "element 0: hidden state\n",
    "element 1: hidden states at each prior attention block.\n",
    "\n",
    "Looking at the vectors in element 1, it seems the 3rd dimension runs over the \"heads\" as there are 25 for this model. Below just check to make sure the same is true for the smaller gpt2 model. It seems to match, 12 heads, final hidden state is 768."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare some cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2Model were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model2 = GPT2Model.from_pretrained('gpt2') \n",
    "tokenizer2 = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 768])\n",
      "12\n",
      "torch.Size([2, 1, 12, 3, 64])\n",
      "torch.Size([2, 1, 12, 3, 64])\n"
     ]
    }
   ],
   "source": [
    "tokens2 = tokenizer2.encode(\"I am a great man\", return_tensors='pt')\n",
    "model2.train(False)\n",
    "representation2 = model2(tokens)\n",
    "print(representation2[0].shape)\n",
    "print(len(representation2[1]))\n",
    "print(representation2[1][0].shape)\n",
    "print(representation2[1][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99936974\n"
     ]
    }
   ],
   "source": [
    "tok_pos = tokenizer2.encode(\"I am a great man\", return_tensors='pt')\n",
    "rep_pos = model2(tok_pos)[0].numpy().flatten()\n",
    "tok_neg = tokenizer2.encode(\"I am a terrible man\", return_tensors='pt')\n",
    "rep_neg = model2(tok_neg)[0].numpy().flatten()\n",
    "\n",
    "print(rep_neg.dot(rep_pos)/(norm(rep_neg)*norm(rep_pos)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12, 768])\n",
      "12\n",
      "torch.Size([2, 1, 12, 12, 64])\n",
      "torch.Size([2, 1, 12, 12, 64])\n",
      "torch.Size([1, 5, 768])\n",
      "12\n",
      "torch.Size([2, 1, 12, 5, 64])\n",
      "torch.Size([2, 1, 12, 5, 64])\n"
     ]
    }
   ],
   "source": [
    "tok_pos = tokenizer2.encode(\"I ate dinner at 5 pm, that's a bit strange\", return_tensors='pt')\n",
    "rep_pos = model2(tok_pos)[0].numpy().flatten()\n",
    "tok_neg = tokenizer2.encode(\"I am the walrus\", return_tensors='pt')\n",
    "rep_neg = model2(tok_neg)[0].numpy().flatten()\n",
    "\n",
    "rep_pos = model2(tok_pos)\n",
    "print(rep_pos[0].shape)\n",
    "print(len(rep_pos[1]))\n",
    "print(rep_pos[1][0].shape)\n",
    "print(rep_pos[1][1].shape)\n",
    "\n",
    "rep_pos = model2(tok_neg)\n",
    "print(rep_pos[0].shape)\n",
    "print(len(rep_pos[1]))\n",
    "print(rep_pos[1][0].shape)\n",
    "print(rep_pos[1][1].shape)\n",
    "#print(rep_neg.dot(rep_pos)/(norm(rep_neg)*norm(rep_pos)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, this is a bit more work than I thought, the `hidden state` is the attention representation for each token (I guess in the previous sentence `am a` must be ignored, which is why the second dimension is length 3). If we want \"sentence representations\" then we will need to figure out how these are combined in the \"decoder\" I guess."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ToDo\n",
    "1. Error messages keep saying we should train a model before running any inference, that's pretty weird, wonder if that's actually neccessary. Need to look into that.\n",
    "2. If I want to decode the hidden state of this sentence, I wonder if I can just instantiate one of those `LMHead`s and then try to decode the difference vectors\n",
    "3. What's the natural way to compose the attention reps for each word?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
